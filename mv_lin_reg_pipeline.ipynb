{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Multivariate Linear Regression Pipeline to Predict House Sale Prices\n",
    "\n",
    "This project works with housing data for the city of Ames, Iowa, United States from 2006 to 2010. You can read more about why the data was collected here https://doi.org/10.1080/10691898.2011.11889627. You can also read about the different columns in the data here https://s3.amazonaws.com/dq-content/307/data_description.txt. The pipeline\n",
    "- cleans the data\n",
    "- selects the most promising features, including both numerical and categorical features converted to dummy variables\n",
    "- trains and tests a multivariate linear regression model on the selected features, cross-validating with sklearn's KFold function\n",
    "- is written in such a way that it could be relatively easily adapted to another dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = pd.read_csv('AmesHousing.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features(data):\n",
    "    train = housing_data.copy()\n",
    "    \n",
    "    ## Drop any column with more than some cutoff-percentage missing values\n",
    "    thresh = .05\n",
    "    train.dropna(thresh = len(train) * thresh, axis = 1, inplace = True)\n",
    "\n",
    "    # Series object: column name -> number of missing values\n",
    "    num_missing = train.isnull().sum()\n",
    "\n",
    "    # Filter series to columns containing >5% missing values\n",
    "    drop_missing_cols = num_missing[\n",
    "        (num_missing > len(train) * thresh)].sort_values()\n",
    "\n",
    "    # Drop those columns from the data frame. Note the use of the .index\n",
    "    train = train.drop(drop_missing_cols.index, axis=1)\n",
    "\n",
    "    # Drop any other columns we don't want in the model\n",
    "    train = train.drop(['PID', 'Order'], axis=1)\n",
    "\n",
    "\n",
    "    ### Transform features into proper format, i.e. numerical to categorical,\n",
    "    ### scaling numerical, filling missing values, etc.\n",
    "\n",
    "    ## For columns with fewer than 5% missing values, fill with mean\n",
    "    # Get null counts for each column\n",
    "    train_null_counts = train.isnull().sum()\n",
    "\n",
    "    # Select columns with <5% missing values\n",
    "    df_missing_values = train_null_counts[\n",
    "        (train_null_counts < 0.05 * len(train))]\n",
    "    df_missing_values = train[df_missing_values.index]\n",
    "\n",
    "\n",
    "    ## Impute missing values\n",
    "    # Compute column-wise missing value counts\n",
    "    num_missing = train.select_dtypes(\n",
    "        include=['int', 'float']).isnull().sum()\n",
    "    fixable_numeric_cols = num_missing[\n",
    "        (num_missing < len(train) * thresh) & (num_missing > 0)].sort_values()\n",
    "\n",
    "    # Compute the most common value for each column\n",
    "    replacement_values_dict = train[\n",
    "        fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n",
    "\n",
    "    # Use `pd.DataFrame.fillna()` to replace missing values.\n",
    "    train = train.fillna(replacement_values_dict)\n",
    "\n",
    "\n",
    "    ## Drop any text columns with >1 missing value\n",
    "    # Create series object: column name -> number of missing values\n",
    "    text_mv_counts = train.select_dtypes(include=['object']).isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "    # Filter series to columns containing *any* missing values\n",
    "    drop_missing_cols_2 = text_mv_counts[text_mv_counts > 0]\n",
    "\n",
    "    train = train.drop(drop_missing_cols_2.index, axis=1)\n",
    "\n",
    "\n",
    "    ## Create new features by combining other features\n",
    "    years_sold = train['Yr Sold'] - train['Year Built']\n",
    "    neg_indices = set(years_sold.index[years_sold < 0].tolist())\n",
    "\n",
    "    years_since_remod = train['Yr Sold'] - train['Year Remod/Add']\n",
    "    neg_indices.update(\n",
    "        years_since_remod.index[years_since_remod < 0].tolist())\n",
    "\n",
    "    ## Create new columns\n",
    "    train['Years Before Sale'] = years_sold\n",
    "    train['Years Since Remod'] = years_since_remod\n",
    "\n",
    "    ## Drop rows with negative values for both of these new features\n",
    "    train = train.drop(neg_indices, axis=0)\n",
    "\n",
    "    ## No longer need original year columns\n",
    "    train = train.drop([\"Year Built\", \"Year Remod/Add\"], axis = 1)\n",
    "\n",
    "    ## Drop any column that leaks info about the sale, such as Sale Year\n",
    "    train = train.drop(\n",
    "        ['Mo Sold', 'Yr Sold', 'Sale Type', 'Sale Condition'], axis=1)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(df, coeff_threshold=0.4, uniq_threshold=10):\n",
    "    numerical_df = df.select_dtypes(include=['int', 'float'])\n",
    "    abs_corr_coeffs = numerical_df.corr()['SalePrice'].abs().sort_values()\n",
    "    df = df.drop(\n",
    "        abs_corr_coeffs[abs_corr_coeffs < coeff_threshold].index, axis=1)\n",
    "    \n",
    "    # All categorical columns in the original dataset\n",
    "    nominal_features = [\n",
    "        \"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \n",
    "        \"Land Contour\", \"Lot Config\", \"Neighborhood\", \"Condition 1\", \n",
    "        \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \n",
    "        \"Roof Matl\", \"Exterior 1st\", \"Exterior 2nd\", \"Mas Vnr Type\", \n",
    "        \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
    "        \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]\n",
    "    \n",
    "    # Reduce list to columns we still have\n",
    "    transform_cat_cols = []\n",
    "    for col in nominal_features:\n",
    "        if col in df.columns:\n",
    "            transform_cat_cols.append(col)\n",
    "\n",
    "    # Drop categorical columns with too many unique values; default 10\n",
    "    uniqueness_counts = df[transform_cat_cols].apply(\n",
    "        lambda col: len(col.value_counts())).sort_values()\n",
    "    drop_nonuniq_cols = uniqueness_counts[uniqueness_counts > 10].index\n",
    "    df = df.drop(drop_nonuniq_cols, axis=1)\n",
    "    \n",
    "    # Select just the remaining text columns and convert to categorical\n",
    "    text_cols = df.select_dtypes(include=['object'])\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].astype('category')\n",
    "    \n",
    "    # Create dummy columns and add back to the dataframe!\n",
    "    df = pd.concat([df, pd.get_dummies(df.select_dtypes(\n",
    "        include=['category']))], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(df, k=0):\n",
    "    numeric_df = df.select_dtypes(include=['integer', 'float'])\n",
    "    features = numeric_df.columns.drop(\"SalePrice\")\n",
    "    lr = LinearRegression()\n",
    "    \n",
    "    # Holdout validation\n",
    "    if k == 0:\n",
    "        # Splits dataset into training and testing set\n",
    "        train = df[:1460]\n",
    "        test = df[1460:]\n",
    "\n",
    "        # Define regression training set\n",
    "        X = train[features]\n",
    "        y = train['SalePrice']\n",
    "\n",
    "        # Train model using selected features from training set\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "\n",
    "        # Predict sale prices on testing set using features\n",
    "        test_pr = reg.predict(test[features])\n",
    "\n",
    "        # Calculate RMSE for test prediction set\n",
    "        test_rmse = np.sqrt(mean_squared_error(test['SalePrice'], test_pr))\n",
    "\n",
    "        return test_rmse\n",
    "    \n",
    "    # Simple cross validation\n",
    "    elif k == 1:\n",
    "        # Randomize *all* rows (frac=1) from `df` and return\n",
    "        shuffled_df = df.sample(frac=1, )\n",
    "        fold_one = df[:1460]\n",
    "        fold_two = df[1460:]\n",
    "        \n",
    "        # Splits dataset into two folds\n",
    "        fold_one = df[:1460]\n",
    "        fold_two = df[1460:]\n",
    "\n",
    "        # Define regression training set using first fold\n",
    "        X = fold_one[features]\n",
    "        y = fold_one['SalePrice']\n",
    "\n",
    "        # Train model using selected features from training set\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "\n",
    "        # Predict sale prices on fold_two using features\n",
    "        fold_two_pred = reg.predict(fold_two[features])\n",
    "\n",
    "        # Calculate RMSE for test prediction set\n",
    "        rmse_one = np.sqrt(\n",
    "            mean_squared_error(fold_two['SalePrice'], fold_two_pred))\n",
    "        print('RMSE one: ', rmse_one)\n",
    "        \n",
    "        \n",
    "        # Define regression training set using second fold\n",
    "        X = fold_two[features]\n",
    "        y = fold_two['SalePrice']\n",
    "\n",
    "        # Train model using selected features from training set\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "\n",
    "        # Predict sale prices on fold_one using features\n",
    "        fold_one_pred = reg.predict(fold_one[features])\n",
    "\n",
    "        # Calculate RMSE for test prediction set\n",
    "        rmse_two = np.sqrt(\n",
    "            mean_squared_error(fold_one['SalePrice'], fold_one_pred))\n",
    "        print('RMSE two: ', rmse_two)\n",
    "\n",
    "        avg_rmse = np.mean([rmse_one, rmse_two])\n",
    "        return avg_rmse\n",
    "    \n",
    "    else:\n",
    "        # Instantiate sklearn's KFold function\n",
    "        kf = KFold(n_splits=k, shuffle=True)\n",
    "        \n",
    "        rmse_values = []\n",
    "        for train_index, test_index, in kf.split(df):\n",
    "            # Define training and testing set\n",
    "            train = df.iloc[train_index]\n",
    "            test = df.iloc[test_index]\n",
    "            \n",
    "            # Fit model\n",
    "            lr.fit(train[features], train[\"SalePrice\"])\n",
    "            \n",
    "            # Predict on test set\n",
    "            preds = lr.predict(test[features])\n",
    "            \n",
    "            # Calculate and append RMSE to list\n",
    "            rmse = np.sqrt(mean_squared_error(test['SalePrice'], preds))\n",
    "            rmse_values.append(rmse)\n",
    "            \n",
    "        avg_rmse = np.mean(rmse_values)\n",
    "\n",
    "        return avg_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28874.354458496986"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test functions above\n",
    "transform_df = transform_features(housing_data)\n",
    "filtered_df = select_features(transform_df, \n",
    "                              coeff_threshold=0.4, uniq_threshold=10)\n",
    "rmse = train_and_test(filtered_df, k=4)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(df, coeff_threshold=0.3, uniq_threshold=12, k=10):\n",
    "    # Tweak pipeline parameters to find optimal solution\n",
    "    transform_df = transform_features(housing_data)\n",
    "    filtered_df = select_features(\n",
    "        transform_df, coeff_threshold, uniq_threshold)\n",
    "    rmse = train_and_test(filtered_df, k)\n",
    "    print('c: ', coeff_threshold, \n",
    "          ' u: ', uniq_threshold, \n",
    "          ' k: ', k, \n",
    "          '\\nrmse: ', round(rmse, 0))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:  0.3  u:  12  k:  10 \n",
      "rmse:  28703.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28703.407322398736"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(housing_data)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
